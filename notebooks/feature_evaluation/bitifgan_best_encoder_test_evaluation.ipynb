{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the BiTiFGAN encoder with the best validation performance on the test set\n",
    "\n",
    "We select the checkpoint from the BiTiFGAN training which yielded the representations with the highest performance on the validation set and test the representations' performance on the SC09 test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.join(\"/\", \"home\", \"c-matsty\", \"Bi-TiFGAN---TensorFlow-1.14\", \"src\"))\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from gantools.model import BiSpectrogramGAN\n",
    "from gantools.gansystem import GANsystem\n",
    "from hyperparams.tifgan_hyperparams import get_hyperparams\n",
    "from feature_evaluation.utils import load_data, load_data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"/media\", \"datastore\", \"c-matsty-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join(data_dir, \"datasets\", \"SpeechCommands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(dataset_dir, \"SpeechCommands_Preproc_2_training\")\n",
    "train_input_path = os.path.join(train_dir, \"input_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.join(dataset_dir, \"SpeechCommands_Preproc_2_test\")\n",
    "test_input_path = os.path.join(test_dir, \"input_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels_path = os.path.join(train_dir, \"labels\")\n",
    "test_labels_path = os.path.join(test_dir, \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define path to BiTiFGAN checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_path = os.path.join(data_dir, \"checkpoints_summaries\", \"bitifgan-results-sc09-run6-512-gradnorm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define path to BiTiFGAN evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = os.path.join(\"..\", \"..\")\n",
    "rf_evaluation_results = os.path.join(results_dir, \"bitifgan_516_gradclip_eval_res_rf_holdoutCV\", \n",
    "                                     \"evaluation_over_time_results.npz\")\n",
    "lr_evaluation_results = os.path.join(results_dir,\"bitifgan_516_gradclip_eval_res_lr_holdoutCV\",\n",
    "                                     \"evaluation_over_time_results.npz\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load evaluation results on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = np.load(rf_evaluation_results)\n",
    "lr_results = np.load(lr_evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get checkpoint with best validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_rf = -1\n",
    "best_score_lr = -1\n",
    "best_score_update_step_rf = 0\n",
    "best_score_update_step_lr = 0\n",
    "\n",
    "for i in rf_results.keys():\n",
    "        rf_results_i = rf_results[i]\n",
    "        lr_results_i = lr_results[i]\n",
    "        \n",
    "        mean_f1_score_rf = np.mean(rf_results_i[:, 2])\n",
    "        mean_f1_score_lr = np.mean(lr_results_i[:, 2])\n",
    "        \n",
    "        if mean_f1_score_rf > best_score_rf:\n",
    "            best_score_rf = mean_f1_score_rf\n",
    "            best_score_update_step_rf = i\n",
    "            \n",
    "        if mean_f1_score_lr > best_score_lr:\n",
    "            best_score_lr = mean_f1_score_lr\n",
    "            best_score_update_step_lr = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update step with performing representations using Logistic Regression: 20000 - Performance 0.5789920902066363\n",
      "Update step with performing representations using Random Forest: 32000 - Performance 0.7113905230233338\n"
     ]
    }
   ],
   "source": [
    "print(\"Update step with performing representations using Logistic Regression: {} - Performance {}\".format(best_score_update_step_lr, best_score_lr))\n",
    "print(\"Update step with performing representations using Random Forest: {} - Performance {}\".format(best_score_update_step_rf, best_score_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/165 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:33<00:00,  4.92it/s]\n"
     ]
    }
   ],
   "source": [
    "X_tr = load_data(train_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:04<00:00,  4.97it/s]\n"
     ]
    }
   ],
   "source": [
    "X_ts = load_data(test_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 3498.33it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 2504.81it/s]\n"
     ]
    }
   ],
   "source": [
    "y_train = load_data_labels(training_labels_path)\n",
    "y_test = load_data_labels(test_labels_path)\n",
    "label_dict = {value: index for index, value in enumerate(np.unique(y_train))}\n",
    "y_train = np.vectorize(label_dict.get)(y_train)\n",
    "y_test = np.vectorize(label_dict.get)(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test best performing representations according to Logistic Regression on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features for both the training and test set with the best performing encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator \n",
      "--------------------------------------------------\n",
      "     The input is of size (?, 100)\n",
      "     0 Full layer with 16384 outputs\n",
      "         Size of the variables: (?, 16384)\n",
      "     Reshape to (?, 8, 4, 512)\n",
      "     1 Deconv layer with 512 channels\n",
      "         Non linearity applied\n",
      "         Size of the variables: (?, 16, 8, 512)\n",
      "     2 Deconv layer with 256 channels\n",
      "         Non linearity applied\n",
      "         Size of the variables: (?, 32, 16, 256)\n",
      "     3 Deconv layer with 128 channels\n",
      "         Non linearity applied\n",
      "         Size of the variables: (?, 64, 32, 128)\n",
      "     4 Deconv layer with 64 channels\n",
      "         Non linearity applied\n",
      "         Size of the variables: (?, 128, 64, 64)\n",
      "     5 Deconv layer with 1 channels\n",
      "         Size of the variables: (?, 256, 128, 1)\n",
      "    Costum non linearity: <function tanh at 0x7f59c23ecf80>\n",
      "     The output is of size (?, 256, 128, 1)\n",
      "--------------------------------------------------\n",
      "\n",
      "Encoder \n",
      "--------------------------------------------------\n",
      "     The data input is of size (?, 256, 128, 1)\n",
      "     0 Conv layer with 64 channels\n",
      "         Size of the variables: (?, 128, 64, 64)\n",
      "     1 Conv layer with 128 channels\n",
      "         Size of the variables: (?, 64, 32, 128)\n",
      "     2 Conv layer with 256 channels\n",
      "         Size of the variables: (?, 32, 16, 256)\n",
      "     3 Conv layer with 512 channels\n",
      "         Size of the variables: (?, 16, 8, 512)\n",
      "     4 Conv layer with 1024 channels\n",
      "         Size of the variables: (?, 8, 4, 1024)\n",
      "     Reshape to (?, 32768)\n",
      "     5 Full layer with 1 outputs\n",
      "     The output is of size (?, 100)\n",
      "--------------------------------------------------\n",
      "\n",
      "Discriminator \n",
      "--------------------------------------------------\n",
      "     The data input is of size (?, 256, 128, 1)\n",
      "     The latent variable input is of size (?, 100)\n",
      "     5 Latent full layer with 50 outputs\n",
      "         Size of the variables: (?, 50)\n",
      "     0 Conv layer with 64 channels\n",
      "         Size of the variables: (?, 128, 64, 64)\n",
      "     1 Conv layer with 128 channels\n",
      "         Size of the variables: (?, 64, 32, 128)\n",
      "     2 Conv layer with 256 channels\n",
      "         Size of the variables: (?, 32, 16, 256)\n",
      "     3 Conv layer with 512 channels\n",
      "         Size of the variables: (?, 16, 8, 512)\n",
      "     4 Conv layer with 1024 channels\n",
      "         Size of the variables: (?, 8, 4, 1024)\n",
      "     Reshape to (?, 32768)\n",
      "     Contenate with latent variables to (?, 32818)\n",
      "     5 Full layer with 516 outputs\n",
      "         Size of the variables: (?, 516)\n",
      "     6 Full layer with 1 outputs\n",
      "     The output is of size (?, 1)\n",
      "--------------------------------------------------\n",
      "\n",
      " Wasserstein loss with gamma_gp=10\n",
      "consistency_contribution 0\n",
      "Add summary for descriptives/mean_real\n",
      "Add summary for descriptives/var_real\n",
      "Add summary for descriptives/min_real\n",
      "Add summary for descriptives/max_real\n",
      "Add summary for descriptives/kurtosis_real\n",
      "Add summary for descriptives/skewness_real\n",
      "Add summary for descriptives/median_real\n",
      "Add summary for descriptives/mean_fake\n",
      "Add summary for descriptives/var_fake\n",
      "Add summary for descriptives/min_fake\n",
      "Add summary for descriptives/max_fake\n",
      "Add summary for descriptives/kurtosis_fake\n",
      "Add summary for descriptives/skewness_fake\n",
      "Add summary for descriptives/median_fake\n",
      "Add summary for descriptives/mean_l2\n",
      "Add summary for descriptives/var_l2\n",
      "Add summary for descriptives/min_l2\n",
      "Add summary for descriptives/max_l2\n",
      "Add summary for descriptives/kurtosis_l2\n",
      "Add summary for descriptives/skewness_l2\n",
      "Add summary for descriptives/median_l2\n",
      "\n",
      "Build the optimizers: \n",
      " * discriminator \n",
      "kwargs:\n",
      "  beta1: 0.5\n",
      "  beta2: 0.9\n",
      "learning_rate: 0.0001\n",
      "optimizer: adam\n",
      "\n",
      " * generator \n",
      "kwargs:\n",
      "  beta1: 0.5\n",
      "  beta2: 0.9\n",
      "learning_rate: 0.0001\n",
      "optimizer: adam\n",
      "\n",
      " * encoder \n",
      "kwargs:\n",
      "  beta1: 0.5\n",
      "  beta2: 0.9\n",
      "learning_rate: 0.0001\n",
      "optimizer: adam\n",
      "\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "generator/0_full/Matrix:0 (float32_ref 100x16384) [1638400, bytes: 6553600]\n",
      "generator/0_full/bias:0 (float32_ref 16384) [16384, bytes: 65536]\n",
      "generator/0_deconv_2d/w:0 (float32_ref 12x3x512x512) [9437184, bytes: 37748736]\n",
      "generator/0_deconv_2d/biases:0 (float32_ref 512) [512, bytes: 2048]\n",
      "generator/1_deconv_2d/w:0 (float32_ref 12x3x256x512) [4718592, bytes: 18874368]\n",
      "generator/1_deconv_2d/biases:0 (float32_ref 256) [256, bytes: 1024]\n",
      "generator/2_deconv_2d/w:0 (float32_ref 12x3x128x256) [1179648, bytes: 4718592]\n",
      "generator/2_deconv_2d/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "generator/3_deconv_2d/w:0 (float32_ref 12x3x64x128) [294912, bytes: 1179648]\n",
      "generator/3_deconv_2d/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "generator/4_deconv_2d/w:0 (float32_ref 12x3x1x64) [2304, bytes: 9216]\n",
      "generator/4_deconv_2d/biases:0 (float32_ref 1) [1, bytes: 4]\n",
      "encoder/0_conv/w:0 (float32_ref 12x3x1x64) [2304, bytes: 9216]\n",
      "encoder/0_conv/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "encoder/1_conv/w:0 (float32_ref 12x3x64x128) [294912, bytes: 1179648]\n",
      "encoder/1_conv/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "encoder/2_conv/w:0 (float32_ref 12x3x128x256) [1179648, bytes: 4718592]\n",
      "encoder/2_conv/biases:0 (float32_ref 256) [256, bytes: 1024]\n",
      "encoder/3_conv/w:0 (float32_ref 12x3x256x512) [4718592, bytes: 18874368]\n",
      "encoder/3_conv/biases:0 (float32_ref 512) [512, bytes: 2048]\n",
      "encoder/4_conv/w:0 (float32_ref 12x3x512x1024) [18874368, bytes: 75497472]\n",
      "encoder/4_conv/biases:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "encoder/out/Matrix:0 (float32_ref 32768x100) [3276800, bytes: 13107200]\n",
      "encoder/out/bias:0 (float32_ref 100) [100, bytes: 400]\n",
      "discriminator/1_latent_full/Matrix:0 (float32_ref 100x50) [5000, bytes: 20000]\n",
      "discriminator/1_latent_full/bias:0 (float32_ref 50) [50, bytes: 200]\n",
      "discriminator/0_conv/w:0 (float32_ref 12x3x1x64) [2304, bytes: 9216]\n",
      "discriminator/0_conv/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "discriminator/1_conv/w:0 (float32_ref 12x3x64x128) [294912, bytes: 1179648]\n",
      "discriminator/1_conv/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "discriminator/2_conv/w:0 (float32_ref 12x3x128x256) [1179648, bytes: 4718592]\n",
      "discriminator/2_conv/biases:0 (float32_ref 256) [256, bytes: 1024]\n",
      "discriminator/3_conv/w:0 (float32_ref 12x3x256x512) [4718592, bytes: 18874368]\n",
      "discriminator/3_conv/biases:0 (float32_ref 512) [512, bytes: 2048]\n",
      "discriminator/4_conv/w:0 (float32_ref 12x3x512x1024) [18874368, bytes: 75497472]\n",
      "discriminator/4_conv/biases:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "discriminator/5_full/Matrix:0 (float32_ref 32818x516) [16934088, bytes: 67736352]\n",
      "discriminator/5_full/bias:0 (float32_ref 516) [516, bytes: 2064]\n",
      "discriminator/out/Matrix:0 (float32_ref 516x1) [516, bytes: 2064]\n",
      "discriminator/out/bias:0 (float32_ref 1) [1, bytes: 4]\n",
      "Total size of variables: 87649072\n",
      "Total bytes of variables: 350596288\n",
      " [*] Reading checkpoints...\n",
      "INFO:tensorflow:Restoring parameters from /media/datastore/c-matsty-data/checkpoints_summaries/bitifgan-results-sc09-run6-512-gradnorm/commands_md64_8k_checkpoints/wgan-20000\n",
      " [*] Reading checkpoints...\n",
      "INFO:tensorflow:Restoring parameters from /media/datastore/c-matsty-data/checkpoints_summaries/bitifgan-results-sc09-run6-512-gradnorm/commands_md64_8k_checkpoints/wgan-20000\n"
     ]
    }
   ],
   "source": [
    "name = 'commands_md64_8k'\n",
    "batch_size = 64\n",
    "with tf.device('/gpu:0'):\n",
    "    params = get_hyperparams(checkpoints_path, name)\n",
    "    biwgan = GANsystem(BiSpectrogramGAN, params)\n",
    "\n",
    "    features_tr = []\n",
    "    with tf.Session() as sess:\n",
    "        biwgan.load(sess=sess, checkpoint=best_score_update_step_lr)\n",
    "\n",
    "        for i in range(0, len(X_tr), batch_size):\n",
    "            x_batch = X_tr[i:i+batch_size]\n",
    "            z = sess.run(biwgan._net.z_real, feed_dict={biwgan._net.X_real: x_batch})\n",
    "            features_tr.append(z)\n",
    "    features_tr = np.vstack(features_tr)\n",
    "    \n",
    "    features_test = []\n",
    "    with tf.Session() as sess:\n",
    "        biwgan.load(sess=sess, checkpoint=best_score_update_step_lr)\n",
    "\n",
    "        for i in range(0, len(X_ts), batch_size):\n",
    "            x_batch = X_ts[i:i+batch_size]\n",
    "            z = sess.run(biwgan._net.z_real, feed_dict={biwgan._net.X_real: x_batch})\n",
    "            features_test.append(z)\n",
    "    features_test = np.vstack(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate sample weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Calculating class_weights based on the training data class labels\n"
     ]
    }
   ],
   "source": [
    "print(\"-Calculating class_weights based on the training data class labels\")\n",
    "label_dict = {value: index for index, value in enumerate(np.unique(y_train))}\n",
    "class_counts = [len(y_train[y_train == i]) for i in label_dict.values()]\n",
    "class_weights = [max(class_counts) / class_count for class_count in class_counts]\n",
    "class_weight_dict = {class_idx: class_weight for class_idx, class_weight in\n",
    "                     zip(label_dict.values(), class_weights)}\n",
    "\n",
    "train_sample_weight = [class_weight_dict[label] for label in y_train]\n",
    "test_sample_weight = [class_weight_dict[label] for label in y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "mean = features_tr.mean()\n",
    "std = features_tr.std()\n",
    "features_tr = (features_tr - mean) / std\n",
    "features_test = (features_test - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(multi_class='multinomial', random_state=0, max_iter=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(features_tr, y_train, sample_weight=train_sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.66      0.63       257\n",
      "           1       0.59      0.59      0.59       270\n",
      "           2       0.64      0.62      0.63       253\n",
      "           3       0.58      0.59      0.59       259\n",
      "           4       0.61      0.60      0.60       248\n",
      "           5       0.63      0.66      0.64       239\n",
      "           6       0.88      0.80      0.84       244\n",
      "           7       0.50      0.54      0.52       267\n",
      "           8       0.54      0.56      0.55       264\n",
      "           9       0.69      0.60      0.64       250\n",
      "\n",
      "    accuracy                           0.62      2551\n",
      "   macro avg       0.63      0.62      0.62      2551\n",
      "weighted avg       0.62      0.62      0.62      2551\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "y_preds = model.predict(features_test)\n",
    "metrics = print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test best performing representations according to Random Forest on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator \n",
      "--------------------------------------------------\n",
      "     The input is of size (?, 100)\n",
      "     0 Full layer with 16384 outputs\n",
      "         Size of the variables: (?, 16384)\n",
      "     Reshape to (?, 8, 4, 512)\n",
      "     1 Deconv layer with 512 channels\n",
      "         Non linearity applied\n",
      "         Size of the variables: (?, 16, 8, 512)\n",
      "     2 Deconv layer with 256 channels\n",
      "         Non linearity applied\n",
      "         Size of the variables: (?, 32, 16, 256)\n",
      "     3 Deconv layer with 128 channels\n",
      "         Non linearity applied\n",
      "         Size of the variables: (?, 64, 32, 128)\n",
      "     4 Deconv layer with 64 channels\n",
      "         Non linearity applied\n",
      "         Size of the variables: (?, 128, 64, 64)\n",
      "     5 Deconv layer with 1 channels\n",
      "         Size of the variables: (?, 256, 128, 1)\n",
      "    Costum non linearity: <function tanh at 0x7f59c23ecf80>\n",
      "     The output is of size (?, 256, 128, 1)\n",
      "--------------------------------------------------\n",
      "\n",
      "Encoder \n",
      "--------------------------------------------------\n",
      "     The data input is of size (?, 256, 128, 1)\n",
      "     0 Conv layer with 64 channels\n",
      "         Size of the variables: (?, 128, 64, 64)\n",
      "     1 Conv layer with 128 channels\n",
      "         Size of the variables: (?, 64, 32, 128)\n",
      "     2 Conv layer with 256 channels\n",
      "         Size of the variables: (?, 32, 16, 256)\n",
      "     3 Conv layer with 512 channels\n",
      "         Size of the variables: (?, 16, 8, 512)\n",
      "     4 Conv layer with 1024 channels\n",
      "         Size of the variables: (?, 8, 4, 1024)\n",
      "     Reshape to (?, 32768)\n",
      "     5 Full layer with 1 outputs\n",
      "     The output is of size (?, 100)\n",
      "--------------------------------------------------\n",
      "\n",
      "Discriminator \n",
      "--------------------------------------------------\n",
      "     The data input is of size (?, 256, 128, 1)\n",
      "     The latent variable input is of size (?, 100)\n",
      "     5 Latent full layer with 50 outputs\n",
      "         Size of the variables: (?, 50)\n",
      "     0 Conv layer with 64 channels\n",
      "         Size of the variables: (?, 128, 64, 64)\n",
      "     1 Conv layer with 128 channels\n",
      "         Size of the variables: (?, 64, 32, 128)\n",
      "     2 Conv layer with 256 channels\n",
      "         Size of the variables: (?, 32, 16, 256)\n",
      "     3 Conv layer with 512 channels\n",
      "         Size of the variables: (?, 16, 8, 512)\n",
      "     4 Conv layer with 1024 channels\n",
      "         Size of the variables: (?, 8, 4, 1024)\n",
      "     Reshape to (?, 32768)\n",
      "     Contenate with latent variables to (?, 32818)\n",
      "     5 Full layer with 516 outputs\n",
      "         Size of the variables: (?, 516)\n",
      "     6 Full layer with 1 outputs\n",
      "     The output is of size (?, 1)\n",
      "--------------------------------------------------\n",
      "\n",
      " Wasserstein loss with gamma_gp=10\n",
      "consistency_contribution 0\n",
      "Add summary for descriptives/mean_real\n",
      "Add summary for descriptives/var_real\n",
      "Add summary for descriptives/min_real\n",
      "Add summary for descriptives/max_real\n",
      "Add summary for descriptives/kurtosis_real\n",
      "Add summary for descriptives/skewness_real\n",
      "Add summary for descriptives/median_real\n",
      "Add summary for descriptives/mean_fake\n",
      "Add summary for descriptives/var_fake\n",
      "Add summary for descriptives/min_fake\n",
      "Add summary for descriptives/max_fake\n",
      "Add summary for descriptives/kurtosis_fake\n",
      "Add summary for descriptives/skewness_fake\n",
      "Add summary for descriptives/median_fake\n",
      "Add summary for descriptives/mean_l2\n",
      "Add summary for descriptives/var_l2\n",
      "Add summary for descriptives/min_l2\n",
      "Add summary for descriptives/max_l2\n",
      "Add summary for descriptives/kurtosis_l2\n",
      "Add summary for descriptives/skewness_l2\n",
      "Add summary for descriptives/median_l2\n",
      "\n",
      "Build the optimizers: \n",
      " * discriminator \n",
      "kwargs:\n",
      "  beta1: 0.5\n",
      "  beta2: 0.9\n",
      "learning_rate: 0.0001\n",
      "optimizer: adam\n",
      "\n",
      " * generator \n",
      "kwargs:\n",
      "  beta1: 0.5\n",
      "  beta2: 0.9\n",
      "learning_rate: 0.0001\n",
      "optimizer: adam\n",
      "\n",
      " * encoder \n",
      "kwargs:\n",
      "  beta1: 0.5\n",
      "  beta2: 0.9\n",
      "learning_rate: 0.0001\n",
      "optimizer: adam\n",
      "\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "generator/0_full/Matrix:0 (float32_ref 100x16384) [1638400, bytes: 6553600]\n",
      "generator/0_full/bias:0 (float32_ref 16384) [16384, bytes: 65536]\n",
      "generator/0_deconv_2d/w:0 (float32_ref 12x3x512x512) [9437184, bytes: 37748736]\n",
      "generator/0_deconv_2d/biases:0 (float32_ref 512) [512, bytes: 2048]\n",
      "generator/1_deconv_2d/w:0 (float32_ref 12x3x256x512) [4718592, bytes: 18874368]\n",
      "generator/1_deconv_2d/biases:0 (float32_ref 256) [256, bytes: 1024]\n",
      "generator/2_deconv_2d/w:0 (float32_ref 12x3x128x256) [1179648, bytes: 4718592]\n",
      "generator/2_deconv_2d/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "generator/3_deconv_2d/w:0 (float32_ref 12x3x64x128) [294912, bytes: 1179648]\n",
      "generator/3_deconv_2d/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "generator/4_deconv_2d/w:0 (float32_ref 12x3x1x64) [2304, bytes: 9216]\n",
      "generator/4_deconv_2d/biases:0 (float32_ref 1) [1, bytes: 4]\n",
      "encoder/0_conv/w:0 (float32_ref 12x3x1x64) [2304, bytes: 9216]\n",
      "encoder/0_conv/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "encoder/1_conv/w:0 (float32_ref 12x3x64x128) [294912, bytes: 1179648]\n",
      "encoder/1_conv/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "encoder/2_conv/w:0 (float32_ref 12x3x128x256) [1179648, bytes: 4718592]\n",
      "encoder/2_conv/biases:0 (float32_ref 256) [256, bytes: 1024]\n",
      "encoder/3_conv/w:0 (float32_ref 12x3x256x512) [4718592, bytes: 18874368]\n",
      "encoder/3_conv/biases:0 (float32_ref 512) [512, bytes: 2048]\n",
      "encoder/4_conv/w:0 (float32_ref 12x3x512x1024) [18874368, bytes: 75497472]\n",
      "encoder/4_conv/biases:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "encoder/out/Matrix:0 (float32_ref 32768x100) [3276800, bytes: 13107200]\n",
      "encoder/out/bias:0 (float32_ref 100) [100, bytes: 400]\n",
      "discriminator/1_latent_full/Matrix:0 (float32_ref 100x50) [5000, bytes: 20000]\n",
      "discriminator/1_latent_full/bias:0 (float32_ref 50) [50, bytes: 200]\n",
      "discriminator/0_conv/w:0 (float32_ref 12x3x1x64) [2304, bytes: 9216]\n",
      "discriminator/0_conv/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "discriminator/1_conv/w:0 (float32_ref 12x3x64x128) [294912, bytes: 1179648]\n",
      "discriminator/1_conv/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "discriminator/2_conv/w:0 (float32_ref 12x3x128x256) [1179648, bytes: 4718592]\n",
      "discriminator/2_conv/biases:0 (float32_ref 256) [256, bytes: 1024]\n",
      "discriminator/3_conv/w:0 (float32_ref 12x3x256x512) [4718592, bytes: 18874368]\n",
      "discriminator/3_conv/biases:0 (float32_ref 512) [512, bytes: 2048]\n",
      "discriminator/4_conv/w:0 (float32_ref 12x3x512x1024) [18874368, bytes: 75497472]\n",
      "discriminator/4_conv/biases:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "discriminator/5_full/Matrix:0 (float32_ref 32818x516) [16934088, bytes: 67736352]\n",
      "discriminator/5_full/bias:0 (float32_ref 516) [516, bytes: 2064]\n",
      "discriminator/out/Matrix:0 (float32_ref 516x1) [516, bytes: 2064]\n",
      "discriminator/out/bias:0 (float32_ref 1) [1, bytes: 4]\n",
      "Total size of variables: 87649072\n",
      "Total bytes of variables: 350596288\n",
      " [*] Reading checkpoints...\n",
      "INFO:tensorflow:Restoring parameters from /media/datastore/c-matsty-data/checkpoints_summaries/bitifgan-results-sc09-run6-512-gradnorm/commands_md64_8k_checkpoints/wgan-32000\n",
      " [*] Reading checkpoints...\n",
      "INFO:tensorflow:Restoring parameters from /media/datastore/c-matsty-data/checkpoints_summaries/bitifgan-results-sc09-run6-512-gradnorm/commands_md64_8k_checkpoints/wgan-32000\n"
     ]
    }
   ],
   "source": [
    "name = 'commands_md64_8k'\n",
    "batch_size = 64\n",
    "with tf.device('/gpu:0'):\n",
    "    params = get_hyperparams(checkpoints_path, name)\n",
    "    biwgan = GANsystem(BiSpectrogramGAN, params)\n",
    "\n",
    "    features_tr = []\n",
    "    with tf.Session() as sess:\n",
    "        biwgan.load(sess=sess, checkpoint=best_score_update_step_rf)\n",
    "\n",
    "        for i in range(0, len(X_tr), batch_size):\n",
    "            x_batch = X_tr[i:i+batch_size]\n",
    "            z = sess.run(biwgan._net.z_real, feed_dict={biwgan._net.X_real: x_batch})\n",
    "            features_tr.append(z)\n",
    "    features_tr = np.vstack(features_tr)\n",
    "    \n",
    "    features_test = []\n",
    "    with tf.Session() as sess:\n",
    "        biwgan.load(sess=sess, checkpoint=best_score_update_step_rf)\n",
    "\n",
    "        for i in range(0, len(X_ts), batch_size):\n",
    "            x_batch = X_ts[i:i+batch_size]\n",
    "            z = sess.run(biwgan._net.z_real, feed_dict={biwgan._net.X_real: x_batch})\n",
    "            features_test.append(z)\n",
    "    features_test = np.vstack(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "mean = features_tr.mean()\n",
    "std = features_tr.std()\n",
    "features_tr = (features_tr - mean) / std\n",
    "features_test = (features_test - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=400, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(features_tr, y_train, sample_weight=train_sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_preds = model.predict(features_test)\n",
    "metrics = print(classification_report(y_test, y_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
